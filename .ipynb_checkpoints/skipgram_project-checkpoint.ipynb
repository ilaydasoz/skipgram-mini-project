{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77caea01-2bed-485f-ae0a-cbf59f17643b",
   "metadata": {
    "id": "77caea01-2bed-485f-ae0a-cbf59f17643b"
   },
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "# Step 2: Implement the Skip-Gram Model\n",
    "# Step 3: Training the Model\n",
    "# Step 4: Evaluation\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad9a62f-d77d-4af0-8cdc-dc8833abd591",
   "metadata": {
    "id": "8ad9a62f-d77d-4af0-8cdc-dc8833abd591"
   },
   "outputs": [],
   "source": [
    "def download_and_unzip():\n",
    "    url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "    if not os.path.exists(\"text8.zip\"):\n",
    "        print(\"Downloading Text8 Dataset...\")\n",
    "        os.system(f\"wget {url}\")\n",
    "    with zipfile.ZipFile(\"text8.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(\"Dataset extracted.\")\n",
    "\n",
    "def preprocess_text():\n",
    "    with open(\"text8\", 'r') as f:\n",
    "        text = f.read(20000000)\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cca13a-94bb-4366-a586-2687e0bacc85",
   "metadata": {
    "id": "f7cca13a-94bb-4366-a586-2687e0bacc85"
   },
   "outputs": [],
   "source": [
    "def build_vocab(words, vocab_size=60000):\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    most_common = word_counts.most_common(vocab_size - 1)\n",
    "\n",
    "    vocab = {\"<UNK>\": 0}  # <UNK> tokens for unknown words\n",
    "    vocab.update({word: idx for idx, (word, _) in enumerate(most_common, start=1)})\n",
    "\n",
    "    indexed_words = [vocab.get(word, 0) for word in words]\n",
    "\n",
    "    return vocab, indexed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3c850-9f9f-44d1-8694-cbfd54722243",
   "metadata": {
    "id": "b0f3c850-9f9f-44d1-8694-cbfd54722243"
   },
   "outputs": [],
   "source": [
    "def generate_skip_gram_pairs(indexed_words, window_size=2):\n",
    "    pairs = []\n",
    "    for i, center_word in enumerate(indexed_words):\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j == 0 or i + j < 0 or i + j >= len(indexed_words):\n",
    "                continue\n",
    "            context_word = indexed_words[i + j]\n",
    "            pairs.append((center_word, context_word))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8937a-21f1-425c-845c-e53bf28af1b0",
   "metadata": {
    "id": "63f8937a-21f1-425c-845c-e53bf28af1b0"
   },
   "outputs": [],
   "source": [
    "def unigram_distribution(indexed_words, vocab_size, alpha=0.75):\n",
    "    word_counts = Counter(indexed_words)\n",
    "    total_count = sum(word_counts.values())\n",
    "\n",
    "    unigram_probs = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        unigram_probs[i] = word_counts[i] / total_count if i in word_counts else 0\n",
    "\n",
    "    smoothed_probs = unigram_probs ** alpha\n",
    "    smoothed_probs /= smoothed_probs.sum()\n",
    "\n",
    "    return smoothed_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6225cee-7457-4299-ad0c-27a540e4cfa2",
   "metadata": {
    "id": "b6225cee-7457-4299-ad0c-27a540e4cfa2"
   },
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs, negative_sampling_probs, num_negatives=5):\n",
    "        self.pairs = pairs\n",
    "        self.negative_sampling_probs = negative_sampling_probs\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        negatives = np.random.choice(\n",
    "            len(self.negative_sampling_probs),\n",
    "            size=self.num_negatives,\n",
    "            p=self.negative_sampling_probs\n",
    "        )\n",
    "\n",
    "        return torch.tensor(center), torch.tensor(context), torch.tensor(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5c536-f42b-42a6-805c-203d16a14b10",
   "metadata": {
    "id": "1eb5c536-f42b-42a6-805c-203d16a14b10"
   },
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "\n",
    "    def forward(self, center, context, negatives):\n",
    "        center_embeds = self.center_embeddings(center)\n",
    "        context_embeds = self.context_embeddings(context)\n",
    "        negative_embeds = self.context_embeddings(negatives)\n",
    "\n",
    "        pos_score = torch.sum(center_embeds * context_embeds, dim=1)\n",
    "\n",
    "        neg_score = torch.bmm(negative_embeds, center_embeds.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150fa523-0a7a-4db7-8057-f74d99c1951d",
   "metadata": {
    "id": "150fa523-0a7a-4db7-8057-f74d99c1951d"
   },
   "outputs": [],
   "source": [
    "def train_skip_gram_model(pairs, negative_sampling_probs, vocab_size, embedding_dim=100, batch_size=1024, epochs=10):\n",
    "    dataset = SkipGramDataset(pairs, negative_sampling_probs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = SkipGramModel(vocab_size, embedding_dim).cuda()\n",
    "    optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (center, context, negatives) in enumerate(dataloader):\n",
    "            center = center.cuda()\n",
    "            context = context.cuda()\n",
    "            negatives = negatives.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                pos_score, neg_score = model(center, context, negatives)\n",
    "\n",
    "                pos_labels = torch.ones_like(pos_score)  \n",
    "                neg_labels = torch.zeros_like(neg_score)  \n",
    "\n",
    "                # calculating loss\n",
    "                pos_loss = criterion(pos_score, pos_labels)\n",
    "                neg_loss = criterion(neg_score.view(-1), neg_labels.view(-1))\n",
    "                loss = pos_loss + neg_loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # print loss every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed with Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb5f2d-5c9c-4be3-9b8a-2a47e16a85ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "aeeb5f2d-5c9c-4be3-9b8a-2a47e16a85ed",
    "outputId": "9a859025-b884-4c0f-e5ae-7ea8cf91bf91"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "download_and_unzip()\n",
    "words = preprocess_text()\n",
    "vocab, indexed_words = build_vocab(words)\n",
    "skip_gram_pairs = generate_skip_gram_pairs(indexed_words)\n",
    "negative_sampling_probs = unigram_distribution(indexed_words, len(vocab))\n",
    "\n",
    "trained_model = train_skip_gram_model(skip_gram_pairs, negative_sampling_probs, len(vocab))\n",
    "\n",
    "# to keep session active on google colab\n",
    "while True:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uQvBAjw-PK6G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "uQvBAjw-PK6G",
    "outputId": "334cfe27-07f7-43cc-e2d8-2b07986901ee"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_scores = {1_000_000: 0.0368, 2_000_000: 0.1388, 5_000_000: 0.2360, 10_000_000: 0.2993, 20_000_000: 0.3650}\n",
    "dataset_sizes = list(correlation_scores.keys())\n",
    "scores = list(correlation_scores.values())\n",
    "\n",
    "plt.plot(dataset_sizes, scores, marker='o', linestyle='-', label=\"Spearman's Correlation\", color='orange')\n",
    "plt.title(\"Spearman's Correlation vs Dataset Size \\n\", fontsize=14)\n",
    "plt.xlabel(\"Dataset Size (characters)\", fontsize=12)\n",
    "plt.ylabel(\"Spearman's Correlation\", fontsize=12)\n",
    "plt.xticks(dataset_sizes, [f'{x:,}' for x in dataset_sizes], rotation=45)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4a4b1-cd9f-4410-b3bf-2ff094dc5d62",
   "metadata": {
    "id": "0cf4a4b1-cd9f-4410-b3bf-2ff094dc5d62"
   },
   "source": [
    "\n",
    "### **Evaluation**\n",
    "#### Cosine Similarity Check\n",
    "- High similarity (>0.7) for synonymous or closely related words.\n",
    "- Low similarity (<0.3) for unrelated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90306e-5d4e-471b-9402-b66da38bc1b9",
   "metadata": {
    "id": "cf90306e-5d4e-471b-9402-b66da38bc1b9"
   },
   "outputs": [],
   "source": [
    "def check_word_similarity(word1, word2, model, vocab):\n",
    "    if word1 not in vocab or word2 not in vocab:\n",
    "        print(f\"One or both words ({word1}, {word2}) are not in the vocabulary.\")\n",
    "        return\n",
    "    embedding_matrix = model.center_embeddings.weight.detach().cpu().numpy()\n",
    "    vec1 = embedding_matrix[vocab[word1]]\n",
    "    vec2 = embedding_matrix[vocab[word2]]\n",
    "    normalized_vec1 = vec1 / np.linalg.norm(vec1)\n",
    "    normalized_vec2 = vec2 / np.linalg.norm(vec2)\n",
    "    similarity = np.dot(normalized_vec1, normalized_vec2)\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422f054-93b1-435f-8b4e-3b795e5d8835",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9422f054-93b1-435f-8b4e-3b795e5d8835",
    "outputId": "8940f1e8-c0d2-4a6b-ba18-26e232269af8"
   },
   "outputs": [],
   "source": [
    "check_word_similarity(\"man\", \"woman\", trained_model, vocab) # example similarity comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab35000-17a1-411d-916d-cbeddbe7252f",
   "metadata": {
    "id": "7ab35000-17a1-411d-916d-cbeddbe7252f"
   },
   "source": [
    "### Word similarity using *WordSim-353*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352bd6a-6631-430e-9557-904e7dab8fe4",
   "metadata": {
    "id": "5352bd6a-6631-430e-9557-904e7dab8fe4"
   },
   "outputs": [],
   "source": [
    "def evaluate_word_embeddings(model, vocab, word_sim_file):\n",
    "    pairs = []\n",
    "    with open(word_sim_file, 'r') as f:\n",
    "        next(f)  \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                word1, word2, score = line.split(',')\n",
    "                pairs.append((word1, word2, float(score)))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    model.eval()\n",
    "    embedding_matrix = model.center_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "    similarities, human_scores = [], []\n",
    "    for word1, word2, score in pairs:\n",
    "        if word1 in vocab and word2 in vocab:\n",
    "            idx1, idx2 = vocab[word1], vocab[word2]\n",
    "            vec1, vec2 = embedding_matrix[idx1], embedding_matrix[idx2]\n",
    "            norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)\n",
    "\n",
    "            if norm1 > 0 and norm2 > 0:\n",
    "                normalized_vec1 = vec1 / norm1\n",
    "                normalized_vec2 = vec2 / norm2\n",
    "                similarity = np.dot(normalized_vec1, normalized_vec2)\n",
    "                similarities.append(similarity)\n",
    "                human_scores.append(score)\n",
    "\n",
    "    if similarities and human_scores:\n",
    "        spearman_corr, _ = spearmanr(similarities, human_scores)\n",
    "        return spearman_corr\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e70bc-6f63-42fc-a97e-ee819566bcc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "919e70bc-6f63-42fc-a97e-ee819566bcc8",
    "outputId": "abbffc62-d1b5-47ee-dd79-a93b672eb604"
   },
   "outputs": [],
   "source": [
    "evaluate_word_embeddings(trained_model, vocab, 'combined.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
